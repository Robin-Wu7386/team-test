#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸­è¯æçŸ¥è¯†å›¾è°±çˆ¬è™« - æœ€ç»ˆä¼˜åŒ–ç‰ˆ
ç›´æ¥è¿è¡Œ: python zhongyaocai_spider.py
"""
from scrapy.exceptions import CloseSpider  # <--- æ–°å¢è¿™è¡Œ
import pandas as pd
import scrapy
import re
import os
import json
import hashlib
import requests
import time
from urllib.parse import urljoin, urlparse
from scrapy.crawler import CrawlerProcess

# å¦‚æœç³»ç»Ÿæ”¯æŒ uvloopï¼Œå¯ä»¥åŠ é€Ÿå¼‚æ­¥å¾ªç¯ï¼ˆLinux/Macæœ‰æ•ˆï¼ŒWindowså¿½ç•¥ï¼‰
try:
    import uvloop
    import asyncio

    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
except ImportError:
    pass


class ZhongyaocaiSpider(scrapy.Spider):
    """ä¸­è¯æçˆ¬è™« - é€‚é… zysj.com.cn"""

    name = 'zhongyaocai'
    allowed_domains = ['zysj.com.cn']

    # ğŸš€ ä¼˜åŒ–é…ç½®ï¼šåŠ é€Ÿä¸é‡è¯•
    custom_settings = {
        # === å¹¶å‘è®¾ç½® (åŠ é€Ÿå…³é”®) ===
        'CONCURRENT_REQUESTS': 32,  # å¢åŠ å…¨å±€å¹¶å‘æ•°
        'CONCURRENT_REQUESTS_PER_DOMAIN': 16,  # é’ˆå¯¹è¯¥åŸŸåçš„å¹¶å‘
        'DOWNLOAD_DELAY': 0.2,  # é™ä½å»¶è¿Ÿ (å¯¹æ–¹åçˆ¬ä¸ä¸¥çš„è¯å¯è®¾ä½)
        'AUTOTHROTTLE_ENABLED': True,  # è‡ªåŠ¨é™é€Ÿï¼Œé˜²æ­¢å°IP
        'AUTOTHROTTLE_START_DELAY': 0.2,
        'AUTOTHROTTLE_MAX_DELAY': 5,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 10.0,

        # === é‡è¯•è®¾ç½® (çˆ¬å–å¤±è´¥é‡è¯•) ===
        'RETRY_ENABLED': True,
        'RETRY_TIMES': 3,  # ç½‘é¡µè¯·æ±‚å¤±è´¥é‡è¯•3æ¬¡
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 429],

        # === å…¶ä»–è®¾ç½® ===
        'LOG_LEVEL': 'INFO',
        'FEED_EXPORT_ENCODING': 'utf-8-sig',
        'ROBOTSTXT_OBEY': False,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }

    # æ ‡å‡†å­—æ®µåˆ—è¡¨
    STANDARD_FIELDS = [
        'æ‹¼éŸ³', 'æ‹¼éŸ³æ³¨éŸ³', 'è‹±æ–‡å', 'åˆ«å', 'æ¥æº', 'åŸå½¢æ€', 'ç”Ÿå¢ƒåˆ†å¸ƒ',
        'æ ½åŸ¹', 'æ€§çŠ¶', 'åŒ–å­¦æˆåˆ†', 'è¯ç†ä½œç”¨', 'é‰´åˆ«', 'ç‚®åˆ¶', 'æ€§å‘³',
        'å½’ç»', 'åŠŸèƒ½ä¸»æ²»', 'ç”¨æ³•ç”¨é‡', 'æ³¨æ„', 'é™„æ–¹', 'å„å®¶è®ºè¿°',
        'æ‘˜å½•', 'å‡ºå¤„', 'å¤æ–¹', 'ä¸´åºŠåº”ç”¨', 'åˆ¶å‰‚', 'å¤‡æ³¨', 'æ¯’æ€§'
    ]

    def __init__(self, max_count=20000, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.base_url = 'https://www.zysj.com.cn'
        self.max_count = int(max_count)

        # æ•°æ®å­˜å‚¨
        self.summary_data = []
        self.details_data = []
        self.processed_urls = set()
        self.medicine_counter = 0

        # åˆ›å»ºç›®å½•
        os.makedirs('images', exist_ok=True)
        os.makedirs('output', exist_ok=True)

        # åŠ è½½æ–­ç‚¹
        self._load_checkpoint()

    def _load_checkpoint(self):
        """åŠ è½½æ–­ç‚¹"""
        if os.path.exists('processed_urls.txt'):
            try:
                with open('processed_urls.txt', 'r', encoding='utf-8') as f:
                    self.processed_urls = set(line.strip() for line in f if line.strip())
                self.logger.info(f"å·²åŠ è½½ {len(self.processed_urls)} ä¸ªå·²å¤„ç†URL")
            except Exception:
                pass

    def _save_checkpoint(self, url):
        """ä¿å­˜æ–­ç‚¹"""
        try:
            with open('processed_urls.txt', 'a', encoding='utf-8') as f:
                f.write(url + '\n')
            self.processed_urls.add(url)
        except Exception:
            pass

    def start_requests(self):
        """èµ·å§‹è¯·æ±‚"""
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ï¼Œæœ€å¤§æ•°é‡: {self.max_count}")

        urls = [f'{self.base_url}/zhongyaocai/index.html']
        # æ‰©å¤§èŒƒå›´ä»¥è¦†ç›–æ‰€æœ‰ç´¢å¼•é¡µ
        urls += [f'{self.base_url}/zhongyaocai/index_{i}.html' for i in range(1, 6)]
        urls += [f'{self.base_url}/zhongyaocai/index__{i}.html' for i in range(1, 40)]

        for url in urls:
            yield scrapy.Request(url, callback=self.parse_list, errback=self.errback)

    def parse_list(self, response):
        """è§£æåˆ—è¡¨é¡µ"""
        # âš¡ï¸ æ£€æŸ¥ç‚¹ 1ï¼šå¦‚æœæ•°é‡å¤Ÿäº†ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸å¼ºåˆ¶åœæ­¢
        if self.medicine_counter >= self.max_count:
            raise CloseSpider(f"å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {self.max_count}")

        # æå–è¯æé“¾æ¥
        links = response.css('a[href*="/zhongyaocai/"][href$="/index.html"]')

        for link in links:
            # âš¡ï¸ æ£€æŸ¥ç‚¹ 2ï¼šå¾ªç¯å†…ä¹Ÿè¦æ£€æŸ¥ï¼Œé˜²æ­¢ä¸€é¡µæå–å¤ªå¤š
            if self.medicine_counter >= self.max_count:
                raise CloseSpider(f"å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {self.max_count}")

            href = link.css('::attr(href)').get()
            title = link.css('::attr(title)').get() or link.css('::text').get()

            if not href or '/zhongyaocai/index' in href:
                continue
            if href.count('/') < 2:
                continue

            detail_url = response.urljoin(href)

            if detail_url in self.processed_urls:
                continue

            title = self._clean_text(title) if title else ''
            medicine_id = self._extract_medicine_id(detail_url)

            yield scrapy.Request(
                url=detail_url,
                callback=self.parse_detail,
                meta={'medicine_id': medicine_id, 'title': title},
                errback=self.errback
            )

    def parse_detail(self, response):
        """è§£æè¯¦æƒ…é¡µ"""
        # âš¡ï¸ æ£€æŸ¥ç‚¹ 3ï¼šè¿›æ¥å…ˆæ£€æŸ¥
        if self.medicine_counter >= self.max_count:
            # å…ˆä¿å­˜å†åœæ­¢
            self._save_final_files()
            raise CloseSpider(f"å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {self.max_count}")

        medicine_id = response.meta.get('medicine_id')
        title = response.meta.get('title', '')

        # éªŒè¯æœ‰æ•ˆæ€§
        content = response.css('div#content').get()
        if not content:
            self.logger.warning(f"é¡µé¢ç»“æ„å¼‚å¸¸ï¼Œè·³è¿‡: {response.url}")
            return

        self.medicine_counter += 1
        if self.medicine_counter % 10 == 0:
            print(f"\rğŸŒ¿ è¿›åº¦: {self.medicine_counter}/{self.max_count} | å½“å‰: {title[:10]}", end='', flush=True)

        # 1. ä¸‹è½½å›¾ç‰‡ (å¸¦é‡è¯•)
        image_urls, local_paths = self._extract_images(response, medicine_id)
        downloaded_paths = self._download_images(image_urls, medicine_id)

        # 2. æå–æ•°æ® (æ ¸å¿ƒä¿®æ”¹: ä½¿ç”¨æ‘˜å½•ä½œä¸ºæ¥æº)
        all_source_data = self._extract_all_sources(response)

        # 3. å…œåº•å¤„ç†
        if not all_source_data:
            all_source_data = [{'source': 'æœªçŸ¥æ¥æº', 'data': {'å¤‡æ³¨': 'é¡µé¢ç»“æ„å¼‚å¸¸ï¼Œæœªèƒ½æå–æ•°æ®'}}]

        # 4. æ±‡æ€»æ‰€æœ‰æ¥æºåç§°
        source_names = [s['source'] for s in all_source_data if s['source'] != 'æœªçŸ¥æ¥æº']
        source_list_str = ';'.join(source_names) if source_names else 'æœªçŸ¥æ¥æº'

        # 5. æ„å»ºä¸»è¡¨
        summary_item = {
            'è¯æID': medicine_id,
            'è¯æåç§°': title or medicine_id,
            'æ‹¼éŸ³æ³¨éŸ³': self._get_first_field(all_source_data, 'æ‹¼éŸ³æ³¨éŸ³'),
            'åˆ«å': self._get_first_field(all_source_data, 'åˆ«å'),
            'å›¾ç‰‡æœ¬åœ°è·¯å¾„': ','.join(downloaded_paths),
            'å›¾ç‰‡åŸå§‹URL': ','.join(image_urls),
            'æ¥æºé¡µé¢URL': response.url,
            'æ¥æºæ•°é‡': len(all_source_data),
            'æ¥æºåˆ—è¡¨': source_list_str,
            'é‡‡é›†æ—¶é—´': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # 6. æ„å»ºå‰¯è¡¨
        detail_items = []
        for source_data in all_source_data:
            source_name = source_data['source']
            for field_name, field_value in source_data['data'].items():
                if field_value:
                    detail_items.append({
                        'è¯æID': medicine_id,
                        'è¯æåç§°': title or medicine_id,
                        'æ¥æº': source_name,
                        'å±æ€§åç§°': field_name,
                        'å±æ€§å€¼': field_value,
                    })

        self.summary_data.append(summary_item)
        self.details_data.extend(detail_items)
        self._save_checkpoint(response.url)

        if self.medicine_counter % 50 == 0:
            self._save_progress()
        # âš¡ï¸ æ£€æŸ¥ç‚¹ 4ï¼šå¤„ç†å®Œå½“å‰è¿™æ¡åï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
        if self.medicine_counter >= self.max_count:
            self.logger.info(f"ğŸ›‘ æ•°é‡å·²è¾¾æ ‡ ({self.medicine_counter})ï¼Œæ­£åœ¨åœæ­¢...")
            raise CloseSpider(f"å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {self.max_count}")

    def _extract_all_sources(self, response):
        """
        æå–æ‰€æœ‰æ¥æºæ•°æ®
        é€»è¾‘ï¼šéå† div.section -> æå–æ‰€æœ‰é”®å€¼å¯¹ -> æŸ¥æ‰¾'æ‘˜å½•'å­—æ®µ -> ç”¨'æ‘˜å½•'çš„å€¼ä½œä¸ºè¯¥sectionçš„æ¥æºå
        """
        all_source_data = []

        # 1. æŸ¥æ‰¾æ‰€æœ‰ section
        sections = response.css('div#content div.section')

        if not sections:
            # å¤‡ç”¨ï¼šé€šç”¨æå–
            fields = self._extract_fields_universal(response)
            if fields:
                source_name = fields.get('æ‘˜å½•', 'æœªçŸ¥æ¥æº')
                # æ¸…ç†æ‘˜å½•ä¸­çš„ä¹¦åå·ç­‰
                source_name = self._clean_source_name(source_name)
                all_source_data.append({'source': source_name, 'data': fields})
            return all_source_data

        for section in sections:
            fields = {}
            items = section.css('div.item')

            # æå–è¯¥ section ä¸‹æ‰€æœ‰å±æ€§
            for item in items:
                key = item.css('div.item-name').xpath('string(.)').get(default='').strip()
                value = item.css('div.item-content').xpath('string(.)').get(default='').strip()
                value = re.sub(r'\s+', ' ', value)  # å‹ç¼©ç©ºç™½

                if key and value:
                    fields[self._normalize_field_name(key)] = value

            if fields:
                # === æ ¸å¿ƒä¿®æ”¹ ===
                # ä¼˜å…ˆä½¿ç”¨è¯¥æ®µè½å†…çš„ 'æ‘˜å½•' å­—æ®µä½œä¸ºæ¥æºå
                if 'æ‘˜å½•' in fields:
                    raw_source = fields['æ‘˜å½•']
                    source_name = self._clean_source_name(raw_source)
                else:
                    # å¦‚æœæ²¡æœ‰æ‘˜å½•å­—æ®µï¼Œå°è¯•ä» section æ ‡é¢˜æå– (å…¼å®¹æ—§é€»è¾‘)
                    raw_title = section.css('h2').xpath('string(.)').get(default='').strip()
                    source_name = self._clean_source_name(raw_title)
                    if not source_name:
                        source_name = 'æœªçŸ¥æ¥æº'

                all_source_data.append({
                    'source': source_name,
                    'data': fields
                })

        return all_source_data

    def _clean_source_name(self, text):
        """æ¸…ç†æ¥æºåç§°ï¼šæå–ä¹¦åæˆ–æ¸…ç†æ ‡ç‚¹"""
        if not text:
            return ''

        # 1. å¦‚æœåŒ…å«ä¹¦åå·ï¼Œæå–ä¹¦åå·å†…å®¹
        if 'ã€Š' in text and 'ã€‹' in text:
            match = re.search(r'ã€Š([^ã€‹]+)ã€‹', text)
            if match:
                return f"ã€Š{match.group(1)}ã€‹"

        # 2. å¦‚æœæ˜¯ 'æ¥æºï¼šXXX' æ ¼å¼
        if 'ï¼š' in text:
            text = text.split('ï¼š')[0]
        if ':' in text:
            text = text.split(':')[0]

        # 3. å»é™¤å¤šä½™ç©ºæ ¼
        text = text.strip()

        # 4. è¡¥å……ä¹¦åå·ï¼ˆå¦‚æœçœ‹èµ·æ¥åƒä¹¦åä½†æ²¡ä¹¦åå·ï¼‰
        known_books = ['å…¨å›½ä¸­è‰è¯æ±‡ç¼–', 'ä¸­è¯å¤§è¾å…¸', 'ä¸­åæœ¬è‰', 'æœ¬è‰çº²ç›®', 'å›¾ç»æœ¬è‰']
        if text in known_books:
            return f"ã€Š{text}ã€‹"

        return text

    def _extract_fields_universal(self, response):
        """[å¤‡ç”¨] é€šç”¨å­—æ®µæå–"""
        fields = {}
        content_area = response.css('div#content')
        all_texts = content_area.xpath('.//text()').getall()

        for text in all_texts:
            text = text.strip()
            # åŒ¹é… "å­—æ®µåï¼šå†…å®¹"
            match = re.match(r'^([^ï¼š:]{2,10})[ï¼š:](.+)$', text)
            if match:
                key = match.group(1).strip()
                val = match.group(2).strip()
                if self._is_valid_field(key):
                    fields[self._normalize_field_name(key)] = val
        return fields

    def _normalize_field_name(self, name):
        """æ ‡å‡†åŒ–å­—æ®µå"""
        name = name.strip()
        name = re.sub(r'[ã€ã€‘\[\]ï¼ˆï¼‰()ï¼š:]', '', name)
        mapping = {
            'æ‹¼éŸ³': 'æ‹¼éŸ³æ³¨éŸ³',
            'æ³¨æ„äº‹é¡¹': 'æ³¨æ„',
            'ä¸»æ²»': 'åŠŸèƒ½ä¸»æ²»',
            'åŠŸæ•ˆ': 'åŠŸèƒ½ä¸»æ²»',
        }
        return mapping.get(name, name)

    def _is_valid_field(self, name):
        """éªŒè¯å­—æ®µåæœ‰æ•ˆæ€§"""
        for std in self.STANDARD_FIELDS:
            if name == std or name.startswith(std):
                return True
        return False

    def _get_first_field(self, all_source_data, field_name):
        """è·å–ç¬¬ä¸€ä¸ªéç©ºå­—æ®µå€¼"""
        for src in all_source_data:
            if field_name in src['data'] and src['data'][field_name]:
                return src['data'][field_name]
        return ''

    def _extract_images(self, response, medicine_id):
        """æå–å›¾ç‰‡URL"""
        image_urls = set()
        selectors = ['img::attr(data-original)', 'img::attr(data-src)', 'img::attr(src)']

        for selector in selectors:
            for url in response.css(selector).getall():
                if not url or url.startswith('data:'): continue
                full_url = response.urljoin(url)
                if '/zhongyaocai/' in full_url and not any(x in full_url for x in ['logo', 'icon']):
                    image_urls.add(full_url)

        image_urls = list(image_urls)[:5]
        local_paths = []
        for i, url in enumerate(image_urls, 1):
            ext = os.path.splitext(urlparse(url).path)[1] or '.jpg'
            local_paths.append(f"images/{medicine_id}_{i}{ext}")
        return image_urls, local_paths

    def _download_images(self, image_urls, medicine_id):
        """å›¾ç‰‡ä¸‹è½½ï¼ˆå¸¦3æ¬¡é‡è¯•ï¼‰"""
        downloaded = []
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

        for i, url in enumerate(image_urls, 1):
            ext = os.path.splitext(urlparse(url).path)[1] or '.jpg'
            if ext.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = '.jpg'
            filename = f"images/{medicine_id}_{i}{ext}"

            if os.path.exists(filename):
                downloaded.append(filename)
                continue

            # ğŸ”„ é‡è¯•å¾ªç¯
            for attempt in range(3):
                try:
                    resp = requests.get(url, headers=headers, timeout=10)
                    if resp.status_code == 200 and len(resp.content) > 500:
                        with open(filename, 'wb') as f:
                            f.write(resp.content)
                        downloaded.append(filename)
                        break  # æˆåŠŸï¼Œè·³å‡ºå¾ªç¯
                except Exception:
                    if attempt < 2: time.sleep(1)  # å¤±è´¥ç­‰å¾…

        return downloaded

    def _extract_medicine_id(self, url):
        """ä»URLæå–ID"""
        path = urlparse(url).path
        parts = path.strip('/').split('/')
        if len(parts) >= 2 and parts[0] == 'zhongyaocai':
            return parts[1].lower()
        return hashlib.md5(url.encode()).hexdigest()[:10]

    def _clean_text(self, text):
        return re.sub(r'\s+', ' ', str(text).strip())

    def _save_progress(self):
        """ä¿å­˜ä¸´æ—¶è¿›åº¦æ–‡ä»¶ï¼ˆé˜²æ­¢æ„å¤–ä¸­æ–­ï¼‰"""
        try:
            if self.summary_data:
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆå¿«ï¼Œåªç”¨äºæ¢å¤ï¼‰
                pd.DataFrame(self.summary_data).to_csv('output/medicines_summary_temp.csv', index=False,
                                                       encoding='utf-8-sig')
                pd.DataFrame(self.details_data).to_csv('output/medicines_details_temp.csv', index=False,
                                                       encoding='utf-8-sig')

                # æ¯500æ¡ä¿å­˜ä¸€æ¬¡Excelï¼ˆå› ä¸ºExcelä¿å­˜æ…¢ï¼‰
                if self.medicine_counter % 500 == 0:
                    pd.DataFrame(self.summary_data).to_excel('output/medicines_summary_temp.xlsx', index=False)
                    pd.DataFrame(self.details_data).to_excel('output/medicines_details_temp.xlsx', index=False)

                if self.medicine_counter % 100 == 0:
                    self.logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸´æ—¶æ–‡ä»¶ï¼ˆ{self.medicine_counter}æ¡ï¼‰")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    def errback(self, failure):
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")

    def closed(self, reason):
        """çˆ¬è™«ç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ–‡ä»¶"""
        print(f"\nâœ… çˆ¬å–ç»“æŸ! å…±å¤„ç† {self.medicine_counter} æ¡æ•°æ®")

        if not self.summary_data:
            print("âš ï¸  æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        try:
            # 1. å…ˆä¿å­˜CSVï¼ˆå¿«é€Ÿï¼Œä¿è¯æ•°æ®ä¸ä¸¢ï¼‰
            summary_df = pd.DataFrame(self.summary_data)
            details_df = pd.DataFrame(self.details_data)

            csv_success = False
            excel_success = False

            # å…ˆä¿å­˜CSV
            csv_paths = [
                'output/medicines_summary.csv',
                'output/medicines_details.csv'
            ]
            summary_df.to_csv(csv_paths[0], index=False, encoding='utf-8-sig')
            details_df.to_csv(csv_paths[1], index=False, encoding='utf-8-sig')
            csv_success = True
            print(f"ğŸ“Š CSVå·²ä¿å­˜: {len(summary_df)}æ¡æ‘˜è¦, {len(details_df)}æ¡è¯¦æƒ…")

            # 2. å†ä¿å­˜Excel
            excel_paths = [
                'output/medicines_summary.xlsx',
                'output/medicines_details.xlsx'
            ]
            if len(summary_df) <= 1000000:
                summary_df.to_excel(excel_paths[0], index=False)
                details_df.to_excel(excel_paths[1], index=False)
                excel_success = True
                print("âœ… Excelæ–‡ä»¶å·²ä¿å­˜")
            else:
                print("âš ï¸  æ•°æ®é‡è¿‡å¤§ï¼Œå»ºè®®ä½¿ç”¨CSVæ–‡ä»¶")
                excel_success = True  # ä¸ç®—å¤±è´¥ï¼Œåªæ˜¯æ²¡ä¿å­˜Excel

            # 3. ç¡®è®¤æ–‡ä»¶å·²æˆåŠŸä¿å­˜åå†åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            if csv_success:
                temp_files = [
                    'output/medicines_summary_temp.csv',
                    'output/medicines_details_temp.csv',
                    'output/medicines_summary_temp.xlsx',
                    'output/medicines_details_temp.xlsx'
                ]

                deleted_count = 0
                for temp_file in temp_files:
                    try:
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                            deleted_count += 1
                    except:
                        pass

                if deleted_count > 0:
                    print(f"ğŸ§¹ å·²æ¸…ç† {deleted_count} ä¸ªä¸´æ—¶æ–‡ä»¶")

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            print("âš ï¸  ä¸´æ—¶æ–‡ä»¶ä¿ç•™ä»¥ä¾¿æ¢å¤æ•°æ®")


# ========== è¿è¡Œå…¥å£ ==========

def main():
    print("=" * 60)
    print("ğŸš€ ä¸­è¯æå…¨é‡çˆ¬è™«")
    print("=" * 60)

    settings = {
        'LOG_LEVEL': 'INFO',
        'LOG_FILE': 'crawl.log',
    }

    # max_count è®¾ç½®ä¸€ä¸ªå¤§æ•°ä»¥çˆ¬å–å…¨éƒ¨
    process = CrawlerProcess(settings)
    process.crawl(ZhongyaocaiSpider, max_count=20000)
    process.start()


if __name__ == '__main__':
    main()